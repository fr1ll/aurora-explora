{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96b6d216-c767-44f9-9df2-fb14524d01fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from transformers import logging\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler\n",
    "from tqdm.auto import tqdm\n",
    "from torch import autocast\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision import transforms as tfms\n",
    "\n",
    "# For video display:\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "# Supress some unnecessary warnings when loading the CLIPTextModel\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# Set device\n",
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aecfd683-15c3-46b5-805c-3b90ee8496ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a842f0-7f8a-41c4-b808-4913d984ff31",
   "metadata": {},
   "source": [
    "## Load one aurora embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30351c0d-f654-4dc9-b99c-9aaf5602ed8f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/aurora_embeds_3/img_emb/img_emb_0.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m aur_img_embeds \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/aurora_embeds_3/img_emb/img_emb_0.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m aur_img_meta \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/aurora_embeds_3/metadata/metadata_0.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/numpy/lib/npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    403\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 405\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    406\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/aurora_embeds_3/img_emb/img_emb_0.npy'"
     ]
    }
   ],
   "source": [
    "aur_img_embeds = np.load(\"data/aurora_embeds_3/img_emb/img_emb_0.npy\")\n",
    "aur_img_meta = pd.read_parquet(\"data/aurora_embeds_3/metadata/metadata_0.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e36b415-f9d6-4f7f-9043-089629da65f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_aur_img = Image.open(aur_img_meta.iloc[4,0])\n",
    "first_aur_embed = aur_img_embeds[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2fb7ee-022b-4863-9404-a3c14e566e9b",
   "metadata": {},
   "source": [
    "## Load some stuff\n",
    "\n",
    "Lifted from [walk_with_stable_diffusion.ipynb](https://colab.research.google.com/drive/1Ef_3FOJUXNFm2gLl5vMe35A_uCk85kuZ?usp=sharing#scrollTo=nom-hSmvUvDE) by Zach Mueller (thanks!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b28cfc4-dfbf-474e-85e4-4a3c135ff901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the autoencoder model which will be used to decode the latents into image space. \n",
    "vae = AutoencoderKL.from_pretrained(\"stabilityai/stable-diffusion-2\", subfolder=\"vae\")\n",
    "\n",
    "# Load the tokenizer and text encoder to tokenize and encode the text. \n",
    "# tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "# text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "# The UNet model for generating the latents.\n",
    "unet = UNet2DConditionModel.from_pretrained(\"stabilityai/stable-diffusion-2\", subfolder=\"unet\")\n",
    "\n",
    "# The noise scheduler\n",
    "scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "\n",
    "# To the GPU we go!\n",
    "vae = vae.to(torch_device)\n",
    "text_encoder = text_encoder.to(torch_device)\n",
    "unet = unet.to(torch_device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0dd5e2b-4988-45ca-9dc2-6ae934b3f31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/willsa/mambaforge/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3378, in run_code\n",
      "    print()\n",
      "  File \"/tmp/ipykernel_125/2411403535.py\", line 2, in <module>\n",
      "    (batch_size, unet.in_channels, height // 8, width // 8),\n",
      "NameError: name 'batch_size' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/willsa/mambaforge/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 1997, in showtraceback\n",
      "    \"\"\"Show a short message for UsageErrors\n",
      "  File \"/home/willsa/mambaforge/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1112, in structured_traceback\n",
      "    etype, value, tb = sys.exc_info()\n",
      "  File \"/home/willsa/mambaforge/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1006, in structured_traceback\n",
      "  File \"/home/willsa/mambaforge/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 859, in structured_traceback\n",
      "    evalue: Optional[BaseException],\n",
      "  File \"/home/willsa/mambaforge/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 793, in format_exception_as_a_whole\n",
      "    pass\n",
      "  File \"/home/willsa/mambaforge/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 848, in get_records\n",
      "    formatter = None\n",
      "  File \"/home/willsa/mambaforge/lib/python3.10/site-packages/stack_data/core.py\", line 597, in stack_data\n",
      "    yield from collapse_repeated(\n",
      "  File \"/home/willsa/mambaforge/lib/python3.10/site-packages/stack_data/utils.py\", line 84, in collapse_repeated\n",
      "    else:\n",
      "  File \"/home/willsa/mambaforge/lib/python3.10/site-packages/stack_data/core.py\", line 587, in mapper\n",
      "    return cls(f, options)\n",
      "  File \"/home/willsa/mambaforge/lib/python3.10/site-packages/stack_data/core.py\", line 551, in __init__\n",
      "    self.executing = Source.executing(frame_or_tb)\n",
      "  File \"/home/willsa/mambaforge/lib/python3.10/site-packages/executing/executing.py\", line 323, in executing\n",
      "    if sys.version_info >= (3, 5):\n",
      "  File \"/home/willsa/mambaforge/lib/python3.10/site-packages/executing/executing.py\", line 247, in for_frame\n",
      "    for i, line in enumerate(lines)\n",
      "  File \"/home/willsa/mambaforge/lib/python3.10/site-packages/executing/executing.py\", line 275, in for_filename\n",
      "    Returns the `Source` object corresponding to the file the frame is executing in.\n",
      "  File \"/home/willsa/mambaforge/lib/python3.10/site-packages/executing/executing.py\", line 285, in _for_filename_and_lines\n",
      "    ):\n",
      "  File \"/home/willsa/mambaforge/lib/python3.10/site-packages/stack_data/core.py\", line 97, in __init__\n",
      "    return [\n",
      "  File \"/home/willsa/mambaforge/lib/python3.10/site-packages/executing/executing.py\", line 387, in asttokens\n",
      "    # type: (str, T) -> T\n",
      "  File \"/home/willsa/mambaforge/lib/python3.10/site-packages/asttokens/asttokens.py\", line 73, in __init__\n",
      "    self._line_numbers.line_to_offset(*start),\n",
      "  File \"/home/willsa/mambaforge/lib/python3.10/site-packages/asttokens/asttokens.py\", line 86, in mark_tokens\n",
      "    return self._text[start: end]\n",
      "  File \"/home/willsa/mambaforge/lib/python3.10/site-packages/asttokens/mark_tokens.py\", line 61, in visit_tree\n",
      "    util.visit_tree(node, self._visit_before_children, self._visit_after_children)\n",
      "  File \"/home/willsa/mambaforge/lib/python3.10/site-packages/asttokens/util.py\", line 246, in visit_tree\n",
      "    ``par_value`` is as returned from ``previsit()`` of the parent, and ``value`` is as\n",
      "  File \"/home/willsa/mambaforge/lib/python3.10/site-packages/asttokens/mark_tokens.py\", line 87, in _visit_after_children\n",
      "    if util.is_empty_astroid_slice(child):\n",
      "AttributeError: module 'asttokens.util' has no attribute 'is_empty_astroid_slice'\n"
     ]
    }
   ],
   "source": [
    "noise  = torch.randn(\n",
    "  (batch_size, unet.in_channels, height // 8, width // 8),\n",
    "  generator=torch.manual_seed(seed_2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2ca00ab-9249-4845-a019-571770974331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diffuse(text_embeddings, init_noise):\n",
    "\n",
    "    height = 768                        # default height of Stable Diffusion\n",
    "    width = 768                         # default width of Stable Diffusion\n",
    "    num_inference_steps = 20            # Number of denoising steps\n",
    "    guidance_scale = 7.5                # Scale for classifier-free guidance\n",
    "    generator = torch.manual_seed(42)   # Seed generator to create the inital latent noise\n",
    "    batch_size = 1\n",
    "\n",
    "    # Prep Scheduler\n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "    # Prep latents\n",
    "    latents = init_noise\n",
    "    latents = latents.to(torch_device)\n",
    "    latents = latents * scheduler.init_noise_sigma # Scaling (previous versions did latents = latents * self.scheduler.sigmas[0]\n",
    "\n",
    "    # Loop\n",
    "    with autocast(\"cuda\"):\n",
    "        for i, t in tqdm(enumerate(scheduler.timesteps)):\n",
    "            # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "            latent_model_input = torch.cat([latents] * 2)\n",
    "            sigma = scheduler.sigmas[i]\n",
    "            # Scale the latents (preconditioning):\n",
    "            # latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5) # Diffusers 0.3 and below\n",
    "            latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "            # predict the noise residual\n",
    "            with torch.no_grad():\n",
    "                noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "            # perform guidance\n",
    "            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "            # compute the previous noisy sample x_t -> x_t-1\n",
    "            # latents = scheduler.step(noise_pred, i, latents)[\"prev_sample\"] # Diffusers 0.3 and below\n",
    "            latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "\n",
    "    # scale and decode the image latents with vae\n",
    "    latents = 1 / 0.18215 * latents\n",
    "    with torch.no_grad():\n",
    "        image = vae.decode(latents).sample\n",
    "\n",
    "    # Display\n",
    "    image = (image / 2 + 0.5).clamp(0, 1)\n",
    "    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "    images = (image * 255).round().astype(\"uint8\")\n",
    "    pil_images = [Image.fromarray(image) for image in images]\n",
    "    return pil_images[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
